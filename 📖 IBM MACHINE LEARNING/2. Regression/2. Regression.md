# Regression: Numerical
This document outlines the workflow for numerical regression in machine learning, covering preprocessing, model training, evaluation, and advanced optimization techniques. 

## Module 1: Preprocessing
Before feeding data into models, specific preprocessing steps are required to ensure data quality and compatibility.

### 1. Categorical Encoding
Use One Hot Encoding (OHC) to convert categorical information into a numerical format.
Best Practice: Drop the first axis (first category) for a baseline comparison in the ML model.
Verification: Check if OHC indices are identical to the original data index (`X_train_ohc.index == X_train.index.all()`).

### 2. Scaling
Scaling allows models (specifically Lasso and Ridge, though not simple Linear Regression) to perform better or converge faster when features have similar scales.
Tools: `StandardScaler`, `MinMaxScaler` from `sklearn.preprocessing`.
Process:
1. Fit the scaler on the training data.
2. Transform the training data.
3. Transform the test data using the same scaler (do not refit on test data).
Note: When scaling, models are trained on actual $y$ values and scaled $X$. You typically do not need to scale $y\_test$ values.

## Module 2: Training, Splits, and Base Models
### 1. Train and Test Splits
Split data to evaluate performance on unseen data. `random_state` is the seed; setting it allows reproducibility.

**Standard Split:**
```python
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X_data, y_data, 
    test_size=0.3, 
    random_state=42
)
```

**Stratified Split (for imbalanced data):**
Use `StratifiedShuffleSplit` to ensure the train/test sets have the same proportion of class labels as the input dataset.
```python
from sklearn.model_selection import StratifiedShuffleSplit

# Get the split indexes
strat_shuf_split = StratifiedShuffleSplit(n_splits=1, 
                                          test_size=0.3, 
                                          random_state=42)

# Generate indices
train_idx, test_idx = next(strat_shuf_split.split(data[feature_cols], data.Activity))

# Create the dataframes
X_train = data.loc[train_idx, feature_cols]
y_train = data.loc[train_idx, 'Activity']

X_test = data.loc[test_idx, feature_cols]
y_test = data.loc[test_idx, 'Activity']
```

### 2. Linear Regression
The fundamental approach to modeling the relationship between a scalar response and one or more explanatory variables.
```python
from sklearn.linear_model import LinearRegression

LR = LinearRegression()
LR.fit(X_train, y_train)
```

### 3. Testing and Evaluation
Evaluate the model using metrics like Mean Squared Error (MSE).
```python
from sklearn.metrics import mean_squared_error

y_test_pred = LR.predict(X_test)
mean_squared_error(y_test, y_test_pred)
```

### 4. Plotting: Predictions vs Actual
Visualizing the residual sugar (or target variable) predictions against actual values.
```python
import matplotlib.pyplot as plt
import seaborn as sns

sns.set_context('notebook')
sns.set_style('white')
fig = plt.figure(figsize=(6,6))
ax = plt.axes()

# Create dataframe for plotting
ph_test_predict = pd.DataFrame({
    'test': y_test.values,
    'predict': y_test_pred_gr_sugar
}).set_index('test').sort_index()

# Plot
ph_test_predict.plot(marker='o', ls='', ax=ax)
ax.set(xlabel='Test', ylabel='Predict', xlim=(0,35), ylim=(0,35));
```

### 5. Other Regression Approaches
*   **Polynomial Regression**: Defined as linear regression, however, polynomial terms are included to capture non-linear effects of features.
*   **Other Models**:
    *   Logistic Regression (for classification)
    *   K-Nearest Neighbors
    *   Decision Trees
    *   Support Vector Machines
    *   Random Forests
    *   Ensemble Methods
    *   Deep Learning Approaches

## Module 3: Advanced Techniques
### 1. Pipelines
Pipelines allow you to chain multiple steps together. However, each function must have a `fit`/`transform` method, and the last step has `fit`.
```python
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression

# Combine multiple processing steps into a Pipeline
estimator = Pipeline([
    ("scaler", StandardScaler()), 
    ("regression", LinearRegression())
])

estimator.fit(X_train, y_train)
estimator.predict(X_test)
```

### 2. Recursive Feature Selection (RFE)
An approach that combines a model or estimator and a desired number of features. Data must be scaled first.
```python
from sklearn.feature_selection import RFE

# Create an instance of the class
# Select top 5 features
rfeMod = RFE(estimator, n_features_to_select=5) 

# Fit the instance on the data and then predict
rfeMod = rfeMod.fit(X_train, y_train)
y_predict = rfeMod.predict(X_test)
```

### 3. Cross Validation
*   **K-Fold**: Creates K equal-sized folds. Trains on K-1 folds and tests on the remaining fold, repeats K times using a different fold as the test set each time, and then averages performance across all K iterations.
*   **Stratified K-Fold**: K-fold cross-validation with representative samples (preserves percentage of samples for each class).
*   **Leave One Out**: Uses every data point as a separate test set.

**Syntax:**
```python
from sklearn.model_selection import cross_val_score, cross_val_predict

# Calculate scores
cross_val = cross_val_score(model, X_data, y_data, cv=4, scoring='neg_mean_squared_error')

# Get predictions using cross validation
predictions = cross_val_predict(estimator, X, y, cv=kf)
```

### 4. Regularization (Lasso, Ridge, Elastic Net)
#### Lasso (L1 Regularization)
Removes features that contribute the least to predicting the output when combined with other features.
**Pros:**
*   Feature Selection: Automatically removes irrelevant features.
*   Prevents Overfitting: Helps when you have many features.
*   Sparse Models: Useful when you expect only a few variables to be important.
**Cons**: Not ideal for Highly Correlated Features (arbitrarily picks one and ignores others).

**Code Example: Hyperparameter Tuning (Alpha)**
```python
import numpy as np
from sklearn.linear_model import Lasso

# Setup alphas to test
alphas = np.geomspace(1e-9, 1e0, num=10)
scores = []

for alpha in alphas:
    las = Lasso(alpha=alpha, max_iter=100000)
    
    estimator = Pipeline([
        ("scaler", StandardScaler()),
        ("lasso_regression", las)
    ])
    
    predictions = cross_val_predict(estimator, X, y, cv=kf)
    score = r2_score(y, predictions)
    scores.append(score)

# Visualize alphas vs scores
plt.semilogx(alphas, scores, '-o')
plt.xlabel('$\alpha$')
plt.ylabel('$R^2$')
```

#### Ridge (L2 Regularization)
Works like Lasso, except rather than removing features, it reduces their importance. Better for multicollinearity.
**Pros:**
*   Multicollinearity: Works well when features are highly correlated.
*   Prevents Overfitting: Reduces variance in models with many features.
*   Small Datasets: Often better than Lasso; doesn't remove useful predictors.
**Cons**: Not ideal for feature selection (keeps all features, even weak ones).

**Code Example: RidgeCV**
```python
from sklearn.linear_model import RidgeCV

# Define alphas
alphas = [0.005, 0.05, 0.1, 0.3, 1, 3, 5, 10, 15, 30, 80]

# RidgeCV automatically performs Cross Validation to find best alpha
ridgeCV = RidgeCV(alphas=alphas, cv=4).fit(X_train, y_train)

ridgeCV_rmse = rmse(y_test, ridgeCV.predict(X_test))
print(ridgeCV.alpha_, ridgeCV_rmse)
```

#### Elastic Net
Combines both Ridge (L2) and Lasso (L1) to get both feature selection and coefficient shrinkage.
```python
from sklearn.linear_model import ElasticNetCV

# Define l1_ratio (0 is Ridge, 1 is Lasso)
l1_ratios = np.linspace(0.1, 0.9, 9)

elasticNetCV = ElasticNetCV(alphas=alphas2, 
                            l1_ratio=l1_ratios, 
                            max_iter=1e4).fit(X_train, y_train)

elasticNetCV_rmse = rmse(y_test, elasticNetCV.predict(X_test))
print(elasticNetCV.alpha_, elasticNetCV.l1_ratio_, elasticNetCV_rmse)
```

### 5. Grid Search CV
Combines K-Folds and `cross_val_predict` functions to find optimal hyperparameters comprehensively.
```python
from sklearn.model_selection import GridSearchCV

# Same estimator as before
estimator = Pipeline([
    ("polynomial_features", PolynomialFeatures()),
    ("scaler", StandardScaler()),
    ("ridge_regression", Ridge())
])

params = {
    'polynomial_features__degree': [1, 2, 3],
    'ridge_regression__alpha': np.geomspace(4, 20, 30)
}

grid = GridSearchCV(estimator, params, cv=kf)

# Fit on data
grid.fit(X, y)

# Results
print(grid.best_score_, grid.best_params_)
y_predict = grid.predict(X)

# Get coefficients from the best estimator found
grid.best_estimator_.named_steps['ridge_regression'].coef_
```