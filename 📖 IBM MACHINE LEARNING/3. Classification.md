# Classification: Categorical

This document outlines the workflow for categorical classification in machine learning, covering algorithms, evaluation metrics, and advanced ensemble techniques.

## Module 1: Logistic Regression

### Summary

Essentially transforms a linear regression into a model with a sharp boundary between 0 and 1 (using the Sigmoid function).

### Syntax

```python
from sklearn.linear_model import LogisticRegression
  
# Create an instance of the class
# C is the inverse of regularization strength; smaller values specify stronger regularization.
LR = LogisticRegression(penalty='l2', C=10.0)
  
# Fit the instance on the data
LR.fit(X_train, y_train)
  
# Predict
y_predict = LR.predict(X_test)
  
# Can now view the output fitted coefficients
# LR.coef_
  
# Tune regularization parameters with cross-validation
# from sklearn.linear_model import LogisticRegressionCV
```

### Classification Error Metrics

- **Confusion Matrix**: A table that breaks down predictions into four categories: True Positives, True Negatives, False Positives, and False Negatives. Itâ€™s the foundation for calculating other metrics.
- **Accuracy**: Measures how often the model is correct overall. It works best when the classes are balanced but can be misleading if one class dominates.
- **Specificity**: Tells you how good the model is at correctly identifying negative cases. It focuses on avoiding False Positives.
- **Precision**: Measures how many of the predicted positive cases are actually correct. It focuses on minimizing False Positives and is important when false alarms are costly.
- **Recall (Sensitivity)**: Measures how many of the actual positive cases the model correctly identifies. It focuses on minimizing False Negatives and is important when missing positives is costly.

**Key Differences:**

- **Accuracy** gives a general sense of performance.
- **Specificity** is about correctly identifying negatives.
- **Precision** is about the reliability of positive predictions.
- **Recall** is about capturing all actual positives.

### F1 Score

The F1 score is a single metric that balances Precision and Recall. It is the harmonic mean of the two, providing a way to evaluate a model when both false positives and false negatives are important.

- **Use Case**: Especially useful when dealing with imbalanced datasets, where one class significantly outweighs the other.
- **Balance**: It gives equal weight to Precision and Recall, making it ideal when you need to strike a balance between minimizing false positives and false negatives.

### ROC and AUC

- **ROC Curve (Receiver Operating Characteristic)**: A graph that plots the True Positive Rate (Recall) against the False Positive Rate (1 - Specificity) at various threshold settings.
    - It shows how well the model can distinguish between the two classes as the classification threshold changes.
    - A good model will have a curve that climbs steeply toward the top-left corner (high True Positive Rate, low False Positive Rate).
- **AUC (Area Under the Curve)**: Provides a single number to summarize the model's performance.
    - AUC = 1: Perfect classifier (ideal).
    - AUC = 0.5: No better than random guessing.
    - AUC < 0.5: Worse than random guessing.

**Key Points**: The ROC curve visualizes the trade-off between sensitivity (Recall) and specificity across different thresholds. The AUC quantifies the overall ability of the model to differentiate between classes.

### Code: All Metrics in One Function

```python
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.metrics import roc_auc_score, confusion_matrix, precision_recall_fscore_support
  
def measure_error(y_true, y_pred, label):
    return pd.Series({
        'accuracy': accuracy_score(y_true, y_pred),
        'precision': precision_score(y_true, y_pred),
        'recall': recall_score(y_true, y_pred),
        'f1': f1_score(y_true, y_pred),
        'auc': roc_auc_score(y_true, y_pred),
        'name': label
    })
  
# How to do all in 1 line of code (Detailed Output)
from sklearn.metrics import precision_recall_fscore_support as score
from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score
from sklearn.preprocessing import label_binarize
  
metrics = list()
cm = dict()
  
# Precision, recall, f-score from the multi-class support function
precision, recall, fscore, _ = score(y_test, y_pred, average='weighted')
  
# The usual way to calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
  
# ROC-AUC scores can be calculated by binarizing the data
auc = roc_auc_score(label_binarize(y_test, classes=[0,1,2,3,4,5]),
                    y_prob, # needs probability predictions
                    average='weighted')
  
# Last, the confusion matrix
cm = confusion_matrix(y_test, y_pred)
```

## Module 2: K-Nearest Neighbors (KNN)

### Summary

Essentially looks at the k nearest neighbors and uses them to predict the point.

### How to choose K

- **Find elbow**: Plot error rates against K and find the point where improvement diminishes.
- **Rule of thumb**: $k \approx \sqrt{\text{number of features}} + 1$ (ensure it is odd to avoid ties).

### Syntax

```python
from sklearn.neighbors import KNeighborsClassifier
  
# Create instance
KNN = KNeighborsClassifier(n_neighbors=3)
  
# Fit and Predict
KNN.fit(X_train, y_train)
y_predict = KNN.predict(X_test)
```

### Distance Measurements

- **Euclidean Distance (L2)**: Standard straight-line distance ($d = \sqrt{\Delta x^2 + \Delta y^2}$). Adapts well to new training data. With more features, we just square each of these distances.
- **Manhattan Distance (L1)**: Grid-based distance ($d = |\Delta x| + |\Delta y|$). Sum of absolute differences.

### Pros and Cons

- **Pros**:
    - Simple to implement.
    - Adapts well to new training data.
    - Easy to interpret.
- **Cons**:
    - Slow to predict due to many distance calculations (fitting involves storing training data, prediction involves calculation).
    - Does not generate insight into data generating process (no model/lazy learner).
    - Can require lots of memory if data set is large (or as it grows).
    - Fitting is fast, prediction is slow.

## Module 3: Support Vector Machines (SVM)

### Summary

Essentially maximizes the distances between 2 regions and creates a boundary in the middle of it. Penalizes values away from the margin.

### Regularization Parameters

- **Regularization Term**: Penalizes the vector from being too complicated.
- **Gamma**: Increasing gamma reduces regularization.
    - **High Gamma**: Considers points close to the decision boundary (more complex, potential overfitting).
    - **Low Gamma**: Considers points far from the decision boundary (simpler, smoother).
- **C (Penalty)**: A large C term penalizes misclassifications more heavily, not the coefficients. It reduces the margin to allow fewer misclassifications.
    - C means Penalty.

### Syntax & Kernel Visualization

```python
from sklearn.svm import SVC, LinearSVC
  
# Linear SVM
# C is penalty calculated with the error term
LinSVC = LinearSVC(penalty='l2', C=10.0)
LinSVC.fit(X_train, y_train)
  
# SVM with RBF Kernel
# Gamma controls the influence of individual training points
svc_rbf = SVC(kernel='rbf', gamma=0.5, C=1.0)
svc_rbf.fit(X_train, y_train)
  
# Plotting function (Conceptual)
# plot_decision_boundary(svc_rbf, X, y)
# The contour plot on top shows the decision boundary
```

### Kernels & Approximation

- **Kernels**: Uses a similarity function to map higher order relationships.
- **Cons**: Very slow to train with lots of features and data.
- **Nystroem**: A kernel approximation method used to speed up SVMs.

How to choose model (Timing):

```python
from sklearn.kernel_approximation import Nystroem
from sklearn.linear_model import SGDClassifier
import time
  
# Standard RBF SVM (Slow on large data)
svc = SVC(kernel='rbf')
start = time.time()
svc.fit(X, y)
print(f"SVC Time: {time.time() - start}")
  
# Nystroem Approximation (Faster)
nystroem = Nystroem(kernel='rbf')
sgd = SGDClassifier()
  
start = time.time()
# Fit and transform
X_transformed = nystroem.fit_transform(X)
sgd.fit(X_transformed, y)
print(f"Approx Time: {time.time() - start}")
```

## Module 4: Decision Trees

### Summary

Essentially uses binary trees to go down and down to predict which category they belong in. Can be used for regression as well by asking if value < or >.

### Pros and Cons

- **Pros**: Easy to interpret, handles any data category, no preprocessing or scaling required.
- **Cons**: Prone to overfitting (high variance).

### Methods for Splitting (Greedy)

1. **Classification Error**: $E(t) = 1 - \max[p(i|t)]$. Not often used for splitting directly.
2. **Entropy**: Measures disorder. $H(t) = - \sum p(i|t) \log_2(p(i|t))$.
    - Splitting based on entropy allows further splits to occur.
    - Can eventually reach goal of homogeneous nodes.
3. **Gini Index**: Often used in practice. Similar to entropy but without logarithms (faster to compute).
    - $G(t) = 1 - \sum p(i|t)^2$

Note: The algorithm is greedy (chooses the best split at the current node).

### Syntax & Grid Search

```python
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV
  
# Create instance with tree parameters
DTC = DecisionTreeClassifier(criterion='gini', max_features=10, max_depth=5)
  
DTC.fit(X_train, y_train)
y_predict = DTC.predict(X_test)
  
# Check tree properties (node count and max depth)
# DTC.tree_.node_count
# DTC.tree_.max_depth
  
# Grid Search Example for Decision Tree
param_grid = {
    'max_depth': range(1, 20),
    'max_features': range(1, len(X.columns)+1)
}
  
GR = GridSearchCV(DecisionTreeClassifier(random_state=42),
                  param_grid=param_grid,
                  scoring='accuracy',
                  n_jobs=-1)
  
GR = GR.fit(X_train, y_train)
```

### Modeling Tree as an Image

```python
from io import StringIO
from IPython.display import Image
from sklearn.tree import export_graphviz
import pydotplus
  
dot_data = StringIO()
export_graphviz(DTC, out_file=dot_data, filled=True)
graph = pydotplus.graph_from_dot_data(dot_data.getvalue())
Image(graph.create_png())
```

## Module 5: Ensemble Models

Essentially combining models (Bagging or Boosting).

### Bagging vs Boosting

| Feature              | Bagging                       | Boosting                           |
| :------------------- | :---------------------------- | :--------------------------------- |
| **Sampling**         | Bootstrapping sampling        | Re-weighting data                  |
| **Base Trees**       | Created independently         | Created successively               |
| **Data Focus**       | Only data points considered   | Use residuals from previous models |
| **Weighting**        | No weighting used             | Up-weight misclassified points     |
| **Overfitting Risk** | Excess trees will not overfit | Beware of overfitting              |

### 1. Bagging (Bootstrap Aggregating)

Train multiple trees and aggregate results.

- **Mechanism**: Combines the prediction of several trees that were trained on bootstrap samples of the data.
- **Benefits**: Reduces variance in decision trees. Can grow trees in parallel.
- **Problems**: Trees might still be correlated.

**Syntax**:

```python
from sklearn.ensemble import BaggingClassifier
  
# Create an instance
BC = BaggingClassifier(n_estimators=50)
  
# Fit and Predict
BC = BC.fit(X_train, y_train)
y_predict = BC.predict(X_test)
```

### 2. Random Forest

Further de-correlates trees by using random columns (features) and rows (samples) to introduce even more randomness.

**Syntax & Tuning**:

```python
from sklearn.ensemble import RandomForestClassifier
import pandas as pd
  
# Create an instance
# warm_start=True allows adding more trees without retraining from scratch
RF = RandomForestClassifier(oob_score=True, random_state=42, warm_start=True, n_jobs=-1)
  
oob_list = list()
  
# Iterate through all possibilities for number of trees
for n_trees in [15, 20, 30, 40, 50, 100, 150, 200, 300, 400]:
    RF.set_params(n_estimators=n_trees)
    RF.fit(X_train, y_train)
    
    # Get OOB error
    oob_error = 1 - RF.oob_score_
    oob_list.append(pd.Series({'n_trees': n_trees, 'oob': oob_error}))
  
rf_oob_df = pd.concat(oob_list, axis=1).T.set_index('n_trees')
# rf_oob_df.plot(y='oob')
```

### 3. Extra Trees Classifiers

Selects features randomly and creates splits randomly (doesn't choose greedily).

**Syntax**:

```python
from sklearn.ensemble import ExtraTreesClassifier
  
# Create instance
EC = ExtraTreesClassifier(n_estimators=50)
  
EC = EC.fit(X_train, y_train)
y_predict = EC.predict(X_test)
```

**Example (Plotting OOB Error for comparison)**:

```python
import seaborn as sns
  
# Combine the two dataframes into a single one for easier plotting
oob_df = pd.concat([rf_oob_df.rename(columns={'oob': 'RandomForest'}),
                    et_oob_df.rename(columns={'oob': 'ExtraTrees'})], axis=1)
  
sns.set_context('talk')
sns.set_style('white')
ax = oob_df.plot(marker='o', figsize=(14, 7), linewidth=5)
ax.set(ylabel='out-of-bag error')
```

### 4. Boosting

Boosting is a technique where multiple weak models (like shallow decision trees) are combined to create a stronger, more accurate model.

- **Key Idea**: Learn from mistakes made by previous models and improve over time.

#### 0-1 Loss

Not used often as not differentiable, hence hard to optimize.

#### AdaBoost

Uses exponential loss function; as such, is sensitive to outliers.

**Syntax & Grid Search**:

```python
from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV
  
# Base estimator is usually a stump (depth=1)
ABC = AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=1))
  
# Fit
ABC.fit(X_train, y_train)
y_predict = ABC.predict(X_test)
  
# Grid Search
param_grid = {'n_estimators': [100, 150, 200],
              'learning_rate': [0.01, 0.001]}
  
GV_ABC = GridSearchCV(ABC, param_grid=param_grid, scoring='accuracy', n_jobs=-1)
GV_ABC = GV_ABC.fit(X_train, y_train)
```

#### Gradient Boosting

Uses a log likelihood loss function and hence is more robust to outliers.

- **Deviance**: Gradient Boosting loss function. $\log(1 + e^{-margin})$.
- **Learning Rate**: How much you are correcting the model at each step.
- **Subsample**: Use a fraction of the data (Stochastic Gradient Boosting).
- **Max Features**: Max number of features considered when splitting.

**Syntax & Tuning**:

```python
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import accuracy_score
import pandas as pd
  
# Create an instance
GBC = GradientBoostingClassifier(learning_rate=0.1, n_estimators=100)
GBC.fit(X_train, y_train)
  
# Tuning Loop
error_list = list()
tree_list = [15, 25, 50, 100, 200, 400]
  
for n_trees in tree_list:
    # Initialize
    GBC = GradientBoostingClassifier(max_features=5, n_estimators=n_trees, random_state=42)
    
    # Fit
    GBC.fit(X_train, y_train)
    y_pred = GBC.predict(X_test)
    
    # Get error
    error = 1.0 - accuracy_score(y_test, y_pred)
    error_list.append(pd.Series({'n_trees': n_trees, 'error': error}))
  
error_df = pd.concat(error_list, axis=1).T.set_index('n_trees')
```

**How to Save and Load Models (Pickle)**:

```python
import pickle
  
# Save
pickle.dump(GV_GBC, open('gv_gbc.p', 'wb'))
  
# Load
loaded_model = pickle.load(open('gv_gbc.p', 'rb'))
```

### 5. Stacking (Voting Classifier)

Essentially combines all the classification methods. The final prediction can be a vote or another model.

**Syntax**:

```python
from sklearn.ensemble import VotingClassifier
  
# Create instance
# estimators is a list of (name, model) tuples
VC = VotingClassifier(estimators=[('lr', LR), ('rf', RF), ('gbc', GBC)], voting='soft')
  
VC = VC.fit(X_train, y_train)
y_predict = VC.predict(X_test)
```

**Example (Logistic Regression + Gradient Boost)**:

```python
from sklearn.ensemble import VotingClassifier
  
# The combined model
estimators = [('LR', LR), ('GBC', GBC)]
  
# Voting Classifier
VC = VotingClassifier(estimators=estimators, voting='soft')
VC = VC.fit(X_train, y_train)
```

## Module 6: Advanced Topics

### Handling Unbalanced Classes

Steps for unbalanced datasets (Do in order):

1. Do a stratified test-train split.
2. Up or down sample the full dataset (training set only).
3. Build models.
4. Use cross-validation to see which works best.

**Stratified Sampling**:

- Train-test split: use "stratify" option.
- ShuffleSplit -> StratifiedShuffleSplit.
- KFold -> StratifiedKFold -> RepeatedStratifiedKFold.

**Weighting**:

- Many models allow weighted observations (e.g., `class_weight='balanced'`).
- Adjusts loss so total weight is equal across classes.

**Blagging (Balanced Bagging)**:

- If data has limited targets (minority class), standard bagging might miss them.
- **Solution**: Balanced Bagging (Blagging).
- **Process**:
    1. Bootstrap sample from minority class.
    2. Bootstrap sample from majority class (same size as minority).
    3. Combine and train tree.
    4. Repeat to build ensemble.

### Upsampling vs Downsampling

- **Upsample (Random Oversampling)**: Create copies of the minority class.
    - **Cons**: Mitigates some of the excessive weight on the minor class but overfits to repeated rows.
- **Synthetic Oversampling (SMOTE/ADASYN)**:
    - Start with a point in minority class.
    - Choose one of K nearest neighbors.
    - Add a new point between them.
    - **SMOTE**: Synthetic Minority Oversampling Technique.
    - **ADASYN**: Adaptive Synthetic sampling (focuses on harder-to-learn examples).
- **Downsample**: Remove rows from the majority class.
    - **Cons**: Adds tremendous importance to minority class; can wrongly label major class points as minor class.
- **Resample**: Downsample and upsample combined.

### Model Interpretability

- **Global Surrogate Models**: Create an easier, more interpretable model (like a simple Decision Tree) and use it to explain a black box model.
    - Dataset X -> Black Box Model -> Predictions Y.
    - Dataset X -> Interpretable Model (trained to predict Y) -> Predictions Y'.
- **Feature Importance**: Most tree-based models (Random Forest, Gradient Boosting) provide `feature_importances_`.
