## MODULE 1: Introduction to unsupervised learning and K-means
### OVERVIEW
Unsupervised learning: you do not know the outcomes 
Types of Unsupervised learning 
- Clustering: identify unknown structure in data
	- Helps identify groups of similar customers
	- Examples: 
		- K-means, Hierarchical Agglomerative Clustering, DBSCAN, Mean Shift
- Dimensionality Reduction: use structural characteristics to simplify data
	- Can improve both the performance and interpretability of grouping
	- Examples:
	- Principal Component Analysis
	- Non-negative Matrix Factorisation

Flow: 
unlabeled data --fit-> model
new unlabeled data + model --predict-> map new data to model
hel
### CLUSTERING
Use Cases: 
- spam filter, unlabeled data, anomaly detection, customer segmentation
- improve supervised learning

#### K-MEANS
K -> how many clusters there are
##### HOW IT WORKS
[[4.1 K-Means-Diagram]]:
Pick 2 random points
Then select closest ones, then move to the mean of all the points
While the mean changes: 
	Move clusters to nearest centroid
	Move centroid the new mean of all points

##### SMART INITIALISATION - K means++
Pick one random point as initial. Then for next cluster centroid select with:
$$
prob = \frac{\operatorname{distance}(x_i)^2}
     {\left( \sum_{i=1}^{n} \operatorname{distance}(x_i) \right)^2}
$$
Rewards furthest distance from all centroids. 

##### HOW TO CHOOSE:
- choose based on how many clusters you need (eg 10 groups)
- ##### INERTIA: 
	- sum of squared distance from each point to its cluster $\sum_{i=1}^{n} (x_i-C_k)^2$
	- smaller value corresponds to tighter clusters 
	- value sensitive to number of points in clusters, increases as more points added
	- USE WHEN clusters have similar number of points
- ##### DISTORTION: 
	- average of squared distance from each point $x_i$ to its cluster $C_k$ $\frac{1}{n}\sum_{i=1}^{n} (x_i-c_k)^2$
	- smaller values correspond to tighter clusters
	- doesn't generally increase as more points are added (relvative to inertia)
	- USE WHEN similarity is more important 
- Create a graph and use elbow method based on inertia or distortion

#### SYNTAX:
```python
# K-Means: The Syntax

# Import the class containing the clustering method
from sklearn.cluster import KMeans

# Create an instance of the class
kmeans = KMeans(n_clusters=3, init='k-means++')

# Fit the instance on the data
kmeans = kmeans.fit(X1)

# Predict clusters for new data
y_predict = kmeans.predict(X2)

# Can also be used in batch mode with MiniBatchKMeans
```




## DIMENTIONALITY REDUCTION
Use Cases: 
- image processing
- image tracking

## MODULE 2: Computational hurdles of clustering algorithms 

Curse of dimensionality: observations tend to be further apart in higher dimensions of space 

Euclidean distance (L2) =1 LÂ² Pythagorean theorem
- useful for coordinate based measurements 
- more sensitive to curse of dimensionality 

Manhattan distance = abs change in x + abs change in y 
- useful for higher dimentions compared to Euclidean distance 
- 
Cosine distance = angle created by 2 points compared to origin
- useful for relationships rather than the values 
- Better for data such as text where location of occurrence is less important 

Jaccard distance = 1 - len(shared)/len(unique)
- Applies to sets 

Curse of dimensionality: if you have too many features and not enough rows of data you will be likely to overfit and the accuracy goes down 

## Module 3: Common clustering algorithms
### Hierarchical agglomerative clustering 
- find closest pair
- Merge into a cluster and merge 
- Continue and repeat the merge ( the clusters can also be merged)

- distance defined based on linkage criteria
- 



