## MODULE 1: Introduction to unsupervised learning and K-means
### OVERVIEW
Unsupervised learning: you do not know the outcomes 
Types of Unsupervised learning 
- Clustering: identify unknown structure in data
	- Helps identify groups of similar customers
	- Examples: 
		- K-means, Hierarchical Agglomerative Clustering, DBSCAN, Mean Shift
- Dimensionality Reduction: use structural characteristics to simplify data
	- Can improve both the performance and interpretability of grouping
	- Examples:
		- Principal Component Analysis
		- Non-negative Matrix Factorisation

Flow: 
unlabeled data --fit-> model
new unlabeled data + model --predict-> map new data to model
hel
### CLUSTERING
Use Cases: 
- spam filter, unlabeled data, anomaly detection, customer segmentation
- improve supervised learning

#### K-MEANS
K -> how many clusters there are
##### HOW IT WORKS
[[4.1 K-Means-Diagram]]:
Pick 2 random points
Then select closest ones, then move to the mean of all the points
While the mean changes: 
	Move clusters to nearest centroid
	Move centroid the new mean of all points

##### SMART INITIALISATION - K means++
Pick one random point as initial. Then for next cluster centroid select with:
$$
prob = \frac{\operatorname{distance}(x_i)^2}
     {\left( \sum_{i=1}^{n} \operatorname{distance}(x_i) \right)^2}
$$
Rewards furthest distance from all centroids. 

##### HOW TO CHOOSE:
- choose based on how many clusters you need (eg 10 groups)
- ##### INERTIA: 
	- sum of squared distance from each point to its cluster $\sum_{i=1}^{n} (x_i-C_k)^2$
	- smaller value corresponds to tighter clusters 
	- value sensitive to number of points in clusters, increases as more points added
	- USE WHEN clusters have similar number of points
- ##### DISTORTION: 
	- average of squared distance from each point $x_i$ to its cluster $C_k$ $\frac{1}{n}\sum_{i=1}^{n} (x_i-c_k)^2$
	- smaller values correspond to tighter clusters
	- doesn't generally increase as more points are added (relvative to inertia)
	- USE WHEN similarity is more important 
- Create a graph and use elbow method based on inertia or distortion

#### SYNTAX:
```python
# K-Means: The Syntax

# Import the class containing the clustering method
from sklearn.cluster import KMeans

# Create an instance of the class
kmeans = KMeans(n_clusters=3, init='k-means++')

# Fit the instance on the data
kmeans = kmeans.fit(X1)

# Predict clusters for new data
y_predict = kmeans.predict(X2)

# Can also be used in batch mode with MiniBatchKMeans
```




## DIMENTIONALITY REDUCTION
Use Cases: 
- image processing
- image tracking

## MODULE 2: Computational hurdles of clustering algorithms 

Curse of dimensionality: observations tend to be further apart in higher dimensions of space 

Euclidean distance (L2) =1 L² Pythagorean theorem
- useful for coordinate based measurements 
- more sensitive to curse of dimensionality 

Manhattan distance = abs change in x + abs change in y 
- useful for higher dimentions compared to Euclidean distance 
- 
Cosine distance = angle created by 2 points compared to origin
- useful for relationships rather than the values 
- Better for data such as text where location of occurrence is less important 

Jaccard distance = 1 - len(shared)/len(unique)
- Applies to sets 

Curse of dimensionality: if you have too many features and not enough rows of data you will be likely to overfit and the accuracy goes down 

## MODULE 3: Common clustering algorithms
### Hierarchical agglomerative clustering 
How it works: 
- find closest pair
- Merge into a cluster and merge 
- Continue and repeat the merge ( the clusters can also be merged)

- distance defined based on linkage criteria
- (Elaborate …)

Stop based on stopping criteria eg if distance between clusters is above a certain criteria 

Linkage types:
Single linkage: minimum pairwise distance between different clusters 
- pro: helps ensure a clear separation of clusters, con: but will not separate cleanly if there is noise between two clusters 
Complete linkage: maximum pairwise distance between clusters
- pro will separate clusters better in noise, tends to break apart larger existing clusters 
Average linkage: average pairwise distance between clusters 
- Pro/con: kind like a combination of both single and complete
Ward linkage: merge based on best inertia (similar to k means)
- pro: 

(example)


### DBSCAN: density based spatial clustering of applications with noise
A true clustering algorithm: can have points that don’t belong to any cluster 

Points are clustered using density of local neighbourhood
- finds core points in high density regions and expands clusters from them 

Algorithm ends when all points are classified as either belonging to a cluster or to noise 

Inputs:
- metric: function to calculate distance
- epsilon: radius of local neighbourhood 
- N_clu: determines density threshold (how many points are in the vicinity)
Labels:
Core: a point which has more than n_clu neighbours in the e-neighbourhood 
Density-reachable: an e-neighbour of a core point than n_clu neighbours itself 
Noise: a point that has no core points in its e-neighbourhood
Clusters: connected core and density reachable points 

Strengths: 
do not need to specify number of clusters 
Allows for noise 
Can handle arbitrary shaped clusters 

Weaknesses:
Requires two parameters
Finding appropriate values of e and n_clu can be difficult 
Does not do well with variation of density of clusters 

### Mean Shift 
A partitioning algorithm that sssigns points to nearest cluster centroid 
Centroid: point of highest local density 
Algorithm ends when all points are assigned to a cluster 

Local density is calculated by weighted mean around each point 

Steps:
- Choose a point and window W 
- Calculate weighted mean in W
- Shift centroid of window to a new mean
- Repeat last 2 steps until convergence (no shift ) until local density maximum(mode) is reached
- Repeat last 4 steps for all data points 
- Datapoints leading to same mode are grouped into same cluster 

Strengths:
- model free does not assume number or shape of clusters
- Can use just one parameter
- Robot to outliers

Weakness: 
- result dependent on window size
- Selection of window size is difficult 
- Can be slow to implement O(m x n^2)

(Syntax)

## MODULE 4: Dimentionality Reduction
### Principal Component Analysis
Reduces several features to single features 

Single value Decomosition

[explain more detail about how SVD helps PCA]

![[Pasted image 20251226201054.png]]



